name: Organize PDFs into Knowledge Graph

on:
  push:
    branches:
      - main
    paths:
      - 'pdfs/**'
  workflow_dispatch:
  pull_request:
    branches:
      - main
    paths:
      - 'pdfs/**'

permissions:
  contents: write
  pull-requests: write

jobs:
  build-knowledge-graph:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y graphviz graphviz-dev
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install networkx matplotlib pypdf pdfplumber spacy python-dotenv
          pip install pygraphviz
          # Download spaCy model for NLP
          python -m spacy download en_core_web_sm
      
      - name: Create knowledge graph script
        run: |
          cat > build_knowledge_graph.py << 'EOF'
          import os
          import json
          import re
          from pathlib import Path
          import networkx as nx
          import matplotlib.pyplot as plt
          from collections import defaultdict
          
          try:
              import spacy
              nlp = spacy.load("en_core_web_sm")
              USE_SPACY = True
          except:
              USE_SPACY = False
              print("Warning: spaCy not available, using simple extraction")
          
          def extract_text_from_file(file_path):
              """Extract text from PDF or text files"""
              try:
                  if file_path.suffix.lower() == '.pdf':
                      # Try pdfplumber first
                      try:
                          import pdfplumber
                          with pdfplumber.open(file_path) as pdf:
                              text = ""
                              for page in pdf.pages:
                                  text += page.extract_text() or ""
                          return text
                      except:
                          pass
                      
                      # Fallback to pypdf
                      try:
                          from pypdf import PdfReader
                          reader = PdfReader(file_path)
                          text = ""
                          for page in reader.pages:
                              text += page.extract_text() or ""
                          return text
                      except:
                          print(f"Warning: Could not extract from PDF {file_path}")
                          return ""
                  else:
                      # Text file
                      with open(file_path, 'r', encoding='utf-8') as f:
                          return f.read()
              except Exception as e:
                  print(f"Error reading {file_path}: {e}")
                  return ""
          
          def extract_entities_and_topics(text, doc_name):
              """Extract entities, topics, and key concepts"""
              entities = set()
              topics = set()
              
              if USE_SPACY:
                  doc = nlp(text[:1000000])  # Limit text size for spaCy
                  
                  # Extract named entities
                  for ent in doc.ents:
                      if ent.label_ in ['PERSON', 'ORG', 'PRODUCT', 'EVENT', 'WORK_OF_ART']:
                          entities.add(ent.text)
                  
                  # Extract noun chunks as topics
                  for chunk in doc.noun_chunks:
                      if len(chunk.text.split()) <= 4:  # Limit phrase length
                          topics.add(chunk.text.lower())
              else:
                  # Simple regex-based extraction
                  # Extract capitalized phrases as potential entities
                  entities = set(re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)+\b', text))
                  
              # Extract topics from headers (lines starting with #)
              header_pattern = r'^#+\s+(.+)$'
              for match in re.finditer(header_pattern, text, re.MULTILINE):
                  topics.add(match.group(1).strip().lower())
              
              # Extract key technical terms (simple heuristic)
              technical_terms = set(re.findall(
                  r'\b(?:[A-Z][a-z]+(?:[A-Z][a-z]+)+|'
                  r'[A-Z]{2,}|'
                  r'(?:machine|deep|neural|natural|artificial)\s+(?:learning|network|intelligence|processing))\b',
                  text
              ))
              topics.update(t.lower() for t in technical_terms)
              
              return list(entities), list(topics)
          
          def build_knowledge_graph(pdfs_dir):
              """Build knowledge graph from PDFs"""
              G = nx.Graph()
              document_data = {}
              all_topics = defaultdict(list)
              
              pdf_files = list(Path(pdfs_dir).glob('*.*'))
              print(f"Found {len(pdf_files)} files to process")
              
              for file_path in pdf_files:
                  if file_path.suffix.lower() not in ['.pdf', '.txt']:
                      continue
                  
                  print(f"Processing {file_path.name}...")
                  doc_name = file_path.stem
                  text = extract_text_from_file(file_path)
                  
                  if not text:
                      continue
                  
                  # Extract entities and topics
                  entities, topics = extract_entities_and_topics(text, doc_name)
                  
                  # Add document node
                  G.add_node(doc_name, type='document', file=file_path.name)
                  
                  # Store document data
                  document_data[doc_name] = {
                      'file': file_path.name,
                      'entities': entities[:20],  # Limit to top entities
                      'topics': topics[:30],      # Limit to top topics
                      'text_length': len(text),
                      'word_count': len(text.split())
                  }
                  
                  # Add topic nodes and edges
                  for topic in topics[:30]:  # Limit topics per document
                      topic_id = f"topic_{topic.replace(' ', '_')}"
                      G.add_node(topic_id, type='topic', label=topic)
                      G.add_edge(doc_name, topic_id, relation='discusses')
                      all_topics[topic].append(doc_name)
                  
                  # Add entity nodes and edges
                  for entity in entities[:20]:  # Limit entities per document
                      entity_id = f"entity_{entity.replace(' ', '_')}"
                      G.add_node(entity_id, type='entity', label=entity)
                      G.add_edge(doc_name, entity_id, relation='mentions')
              
              # Create relationships between documents that share topics
              docs = [node for node, data in G.nodes(data=True) if data.get('type') == 'document']
              for i, doc1 in enumerate(docs):
                  for doc2 in docs[i+1:]:
                      doc1_topics = set(document_data[doc1]['topics'])
                      doc2_topics = set(document_data[doc2]['topics'])
                      common_topics = doc1_topics & doc2_topics
                      
                      if len(common_topics) >= 3:  # At least 3 common topics
                          G.add_edge(doc1, doc2, 
                                   relation='related',
                                   common_topics=list(common_topics)[:5])
              
              return G, document_data
          
          def save_knowledge_graph(G, document_data, output_file='knowledge_graph.json'):
              """Save knowledge graph as JSON"""
              graph_data = {
                  'nodes': [],
                  'edges': [],
                  'documents': document_data,
                  'statistics': {
                      'total_nodes': G.number_of_nodes(),
                      'total_edges': G.number_of_edges(),
                      'total_documents': len(document_data)
                  }
              }
              
              for node, data in G.nodes(data=True):
                  graph_data['nodes'].append({
                      'id': node,
                      **data
                  })
              
              for u, v, data in G.edges(data=True):
                  graph_data['edges'].append({
                      'source': u,
                      'target': v,
                      **data
                  })
              
              with open(output_file, 'w') as f:
                  json.dump(graph_data, f, indent=2)
              
              print(f"Knowledge graph saved to {output_file}")
              return graph_data
          
          def visualize_knowledge_graph(G, output_file='knowledge_graph_visualization.png'):
              """Create visualization of the knowledge graph"""
              plt.figure(figsize=(20, 16))
              
              # Separate nodes by type for different colors
              doc_nodes = [n for n, d in G.nodes(data=True) if d.get('type') == 'document']
              topic_nodes = [n for n, d in G.nodes(data=True) if d.get('type') == 'topic']
              entity_nodes = [n for n, d in G.nodes(data=True) if d.get('type') == 'entity']
              
              # Use spring layout for positioning
              pos = nx.spring_layout(G, k=2, iterations=50, seed=42)
              
              # Draw nodes
              nx.draw_networkx_nodes(G, pos, nodelist=doc_nodes, 
                                    node_color='lightblue', node_size=3000, 
                                    alpha=0.9, label='Documents')
              nx.draw_networkx_nodes(G, pos, nodelist=topic_nodes, 
                                    node_color='lightgreen', node_size=1000, 
                                    alpha=0.7, label='Topics')
              nx.draw_networkx_nodes(G, pos, nodelist=entity_nodes, 
                                    node_color='lightcoral', node_size=800, 
                                    alpha=0.6, label='Entities')
              
              # Draw edges
              nx.draw_networkx_edges(G, pos, alpha=0.3, width=1)
              
              # Draw labels for document nodes only (to avoid clutter)
              doc_labels = {n: n for n in doc_nodes}
              nx.draw_networkx_labels(G, pos, labels=doc_labels, font_size=10, font_weight='bold')
              
              plt.title('PDF Knowledge Graph', fontsize=20, fontweight='bold')
              plt.legend(scatterpoints=1, loc='upper left', fontsize=12)
              plt.axis('off')
              plt.tight_layout()
              plt.savefig(output_file, dpi=150, bbox_inches='tight', 
                         facecolor='white', edgecolor='none')
              print(f"Visualization saved to {output_file}")
              plt.close()
          
          def main():
              pdfs_dir = 'pdfs'
              
              print("Building knowledge graph from PDFs...")
              G, document_data = build_knowledge_graph(pdfs_dir)
              
              print(f"\nKnowledge Graph Statistics:")
              print(f"  Total nodes: {G.number_of_nodes()}")
              print(f"  Total edges: {G.number_of_edges()}")
              print(f"  Total documents: {len(document_data)}")
              
              # Save knowledge graph
              graph_data = save_knowledge_graph(G, document_data)
              
              # Create visualization
              visualize_knowledge_graph(G)
              
              # Print summary
              print("\nDocument Summary:")
              for doc_name, data in document_data.items():
                  print(f"  {doc_name}:")
                  print(f"    Words: {data['word_count']}")
                  print(f"    Topics: {len(data['topics'])}")
                  print(f"    Entities: {len(data['entities'])}")
              
              # Export summary for GitHub Actions
              summary_file = os.environ.get('GITHUB_STEP_SUMMARY', 'summary.md')
              if summary_file:
                  with open(summary_file, 'w') as f:
                      f.write("# Knowledge Graph Summary\n\n")
                      f.write(f"## Statistics\n\n")
                      f.write(f"- **Total Nodes**: {G.number_of_nodes()}\n")
                      f.write(f"- **Total Edges**: {G.number_of_edges()}\n")
                      f.write(f"- **Total Documents**: {len(document_data)}\n\n")
                      f.write(f"## Documents Processed\n\n")
                      for doc_name, data in document_data.items():
                          f.write(f"### {doc_name}\n")
                          f.write(f"- File: `{data['file']}`\n")
                          f.write(f"- Word Count: {data['word_count']}\n")
                          f.write(f"- Topics: {len(data['topics'])}\n")
                          f.write(f"- Entities: {len(data['entities'])}\n\n")
          
          if __name__ == '__main__':
              main()
          EOF
      
      - name: Build knowledge graph
        run: |
          python build_knowledge_graph.py
      
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: knowledge-graph
          path: |
            knowledge_graph.json
            knowledge_graph_visualization.png
      
      - name: Commit and push if changed
        if: github.event_name != 'pull_request'
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add knowledge_graph.json knowledge_graph_visualization.png || true
          git diff --quiet && git diff --staged --quiet || (git commit -m "Update knowledge graph [skip ci]" && git push)
